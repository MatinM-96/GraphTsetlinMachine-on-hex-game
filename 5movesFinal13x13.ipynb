{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87facb02-3fcf-46ea-802a-879ac3d85833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import gc\n",
    "from GraphTsetlinMachine.graphs import Graphs\n",
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.epochs = 10\n",
    "        self.number_of_clauses = 20000\n",
    "        self.T = 16000\n",
    "        self.s = 5.0\n",
    "        self.depth = 10\n",
    "        self.hypervector_size = 4096\n",
    "        self.hypervector_bits = 4\n",
    "        self.message_size = 4096\n",
    "        self.message_bits = 4\n",
    "        self.double_hashing = True\n",
    "        self.max_included_literals = 128\n",
    "        self.batch_size = 500\n",
    "        self.patience = 5\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Args()\n",
    "\n",
    "start_time = time()\n",
    "try:\n",
    "    data = pd.read_csv('datasett/5moves_13x13.csv')\n",
    "    data = data.sample(30000, random_state=seed_value).reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found.\")\n",
    "    exit(-1)\n",
    "end_time = time()\n",
    "print(f\"Loading data took {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Dataset size after sampling: {data.shape}\")\n",
    "\n",
    "board_size = 13\n",
    "cell_columns = [f'cell{row}_{col}' for row in range(board_size) for col in range(board_size)]\n",
    "\n",
    "required_columns = ['winner', 'starting_player'] + cell_columns\n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing columns: {missing_columns}\")\n",
    "    exit(-1)\n",
    "\n",
    "X_df = data[cell_columns]\n",
    "y = data['winner'].values.astype(int)\n",
    "starting_player = data['starting_player'].values.astype(int)\n",
    "\n",
    "if X_df.isnull().values.any():\n",
    "    print(\"Warning: Missing values detected. Filling with 0.\")\n",
    "    X_df = X_df.fillna(0)\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "if not set(unique_labels).issubset({0,1}):\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y = np.array([label_mapping[label] for label in y])\n",
    "    print(\"Labels mapped to:\", label_mapping)\n",
    "\n",
    "def augment_data(X_df, y, sp):\n",
    "    X_aug = X_df.copy()\n",
    "    X_aug = X_aug.iloc[:, ::-1]\n",
    "    y_aug = y.copy()\n",
    "    sp_aug = sp.copy()\n",
    "    X_aug = X_aug.replace({1: -1, -1: 1})\n",
    "    return pd.concat([X_df, X_aug], ignore_index=True), np.concatenate([y, y_aug]), np.concatenate([sp, sp_aug])\n",
    "\n",
    "X_df, y, starting_player = augment_data(X_df, y, starting_player)\n",
    "print(f\"Dataset size after augmentation: {X_df.shape}\")\n",
    "print(\"Class distribution after augmentation:\", Counter(y))\n",
    "\n",
    "train_size = int(len(X_df)*0.8)\n",
    "X_train_df = X_df.iloc[:train_size].reset_index(drop=True)\n",
    "y_train = y[:train_size]\n",
    "sp_train = starting_player[:train_size]\n",
    "X_test_df = X_df.iloc[train_size:].reset_index(drop=True)\n",
    "y_test = y[train_size:]\n",
    "sp_test = starting_player[train_size:]\n",
    "\n",
    "print(f\"X_train shape: {X_train_df.shape}\")\n",
    "print(f\"X_test shape: {X_test_df.shape}\")\n",
    "\n",
    "train_label_counts = Counter(y_train)\n",
    "min_class_size = min(train_label_counts.values())\n",
    "print(\"Training set class distribution before balancing:\", train_label_counts)\n",
    "class_indices = {cls: np.where(y_train == cls)[0] for cls in np.unique(y_train)}\n",
    "selected_indices = np.concatenate([\n",
    "    np.random.choice(indices, min_class_size, replace=False) for indices in class_indices.values()\n",
    "])\n",
    "np.random.shuffle(selected_indices)\n",
    "\n",
    "X_train_df = X_train_df.iloc[selected_indices].reset_index(drop=True)\n",
    "y_train = y_train[selected_indices]\n",
    "sp_train = sp_train[selected_indices]\n",
    "\n",
    "print(\"Balanced training set class distribution:\", Counter(y_train))\n",
    "\n",
    "value_to_symbol = {1:'X', -1:'O', 0:'Empty'}\n",
    "\n",
    "symbol_names = [\n",
    "    'X', 'O', 'Empty',\n",
    "    'StartingPlayer0', 'StartingPlayer1',\n",
    "    'Center', 'Edge', 'Corner',\n",
    "    'Bridge',\n",
    "    'IsCriticalBlock'\n",
    "]\n",
    "\n",
    "for r in range(board_size):\n",
    "    symbol_names.append(f'Row_r{r}')\n",
    "for c in range(board_size):\n",
    "    symbol_names.append(f'Col_c{c}')\n",
    "\n",
    "symbol_names.extend(['DistFromCenter_Near', 'DistFromCenter_Mid', 'DistFromCenter_Far',\n",
    "                     'NeighborX_Low', 'NeighborX_Medium', 'NeighborX_High',\n",
    "                     'NeighborO_Low', 'NeighborO_Medium', 'NeighborO_High',\n",
    "                     'TotalX_Low', 'TotalX_Medium', 'TotalX_High',\n",
    "                     'TotalO_Low', 'TotalO_Medium', 'TotalO_High'])\n",
    "\n",
    "def prepare_graph_data(X_df_batch, sp_series_batch, y_batch):\n",
    "    num_graphs = X_df_batch.shape[0]\n",
    "    num_board_nodes = board_size**2\n",
    "    total_nodes_per_graph = num_board_nodes\n",
    "\n",
    "    graphs = Graphs(\n",
    "        number_of_graphs=num_graphs,\n",
    "        symbols=symbol_names,\n",
    "        hypervector_size=args.hypervector_size,\n",
    "        hypervector_bits=args.hypervector_bits,\n",
    "        double_hashing=args.double_hashing\n",
    "    )\n",
    "\n",
    "    nodes = [(r, c) for r in range(board_size) for c in range(board_size)]\n",
    "    node_id_map = {(r,c): idx for idx,(r,c) in enumerate(nodes)}\n",
    "\n",
    "    directions = [\n",
    "        (-1,0), (-1,1), (0,1),\n",
    "        (1,0), (1,-1), (0,-1)\n",
    "    ]\n",
    "\n",
    "    edges = [[] for _ in range(total_nodes_per_graph)]\n",
    "    n_edges_list = [0 for _ in range(total_nodes_per_graph)]\n",
    "\n",
    "    for r,c in nodes:\n",
    "        node_id = node_id_map[(r,c)]\n",
    "        for dr, dc in directions:\n",
    "            nr, nc = r+dr,c+dc\n",
    "            if 0<=nr<board_size and 0<=nc<board_size:\n",
    "                neighbor_id = node_id_map[(nr,nc)]\n",
    "                edges[node_id].append(neighbor_id)\n",
    "                n_edges_list[node_id]+=1\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        graphs.set_number_of_graph_nodes(graph_id, total_nodes_per_graph)\n",
    "    graphs.prepare_node_configuration()\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        for k in range(total_nodes_per_graph):\n",
    "            graphs.add_graph_node(graph_id, k, n_edges_list[k])\n",
    "    graphs.prepare_edge_configuration()\n",
    "\n",
    "    center_r, center_c = board_size//2, board_size//2\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        row_data = X_df_batch.iloc[graph_id]\n",
    "        board_state = row_data.values.astype(int)\n",
    "        board_state_symbols = [value_to_symbol.get(val,'Empty') for val in board_state]\n",
    "        sp = sp_series_batch[graph_id]\n",
    "        winner = y_batch[graph_id]\n",
    "\n",
    "        total_X = board_state_symbols.count('X')\n",
    "        total_O = board_state_symbols.count('O')\n",
    "\n",
    "        if total_X <= 56:\n",
    "            total_X_property = 'TotalX_Low'\n",
    "        elif total_X <= 112:\n",
    "            total_X_property = 'TotalX_Medium'\n",
    "        else:\n",
    "            total_X_property = 'TotalX_High'\n",
    "\n",
    "        if total_O <= 56:\n",
    "            total_O_property = 'TotalO_Low'\n",
    "        elif total_O <= 112:\n",
    "            total_O_property = 'TotalO_Medium'\n",
    "        else:\n",
    "            total_O_property = 'TotalO_High'\n",
    "\n",
    "        board_state_dict = {(r,c):board_state_symbols[idx] for idx,(r,c) in enumerate(nodes)}\n",
    "\n",
    "        for idx,(r,c) in enumerate(nodes):\n",
    "            sym = board_state_symbols[idx]\n",
    "            graphs.add_graph_node_property(graph_id, idx, sym)\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'StartingPlayer{sp}')\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, total_X_property)\n",
    "            graphs.add_graph_node_property(graph_id, idx, total_O_property)\n",
    "\n",
    "            if r==center_r and c==center_c:\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Center')\n",
    "            elif (r in [0, board_size-1]) and (c in [0, board_size-1]):\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Corner')\n",
    "            else:\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Edge')\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'Row_r{r}')\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'Col_c{c}')\n",
    "\n",
    "            dist_from_center = abs(r-center_r)+abs(c-center_c)\n",
    "            if dist_from_center <=4:\n",
    "                dist_property='DistFromCenter_Near'\n",
    "            elif dist_from_center <=8:\n",
    "                dist_property='DistFromCenter_Mid'\n",
    "            else:\n",
    "                dist_property='DistFromCenter_Far'\n",
    "            graphs.add_graph_node_property(graph_id, idx, dist_property)\n",
    "\n",
    "            neighbor_symbols=[]\n",
    "            for dr,dc in directions:\n",
    "                nr,nc=r+dr,c+dc\n",
    "                if 0<=nr<board_size and 0<=nc<board_size:\n",
    "                    neighbor_sym = board_state_dict.get((nr,nc),'Empty')\n",
    "                    neighbor_symbols.append(neighbor_sym)\n",
    "            num_neighbor_X = neighbor_symbols.count('X')\n",
    "            num_neighbor_O = neighbor_symbols.count('O')\n",
    "\n",
    "            def bin_count(cnt):\n",
    "                if cnt<=2:\n",
    "                    return 'Low'\n",
    "                elif cnt<=4:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'NeighborX_{bin_count(num_neighbor_X)}')\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'NeighborO_{bin_count(num_neighbor_O)}')\n",
    "\n",
    "            if sym in ['X','O']:\n",
    "                for dr1,dc1 in directions:\n",
    "                    nr1,nc1=r+dr1,c+dc1\n",
    "                    nr2,nc2=r+2*dr1,c+2*dc1\n",
    "                    if 0<=nr2<board_size and 0<=nc2<board_size:\n",
    "                        sym1=board_state_dict.get((nr1,nc1),'Empty')\n",
    "                        sym2=board_state_dict.get((nr2,nc2),'Empty')\n",
    "                        if sym1=='Empty' and sym2==sym:\n",
    "                            graphs.add_graph_node_property(graph_id, idx,'Bridge')\n",
    "                            break\n",
    "\n",
    "            if winner==1 and sym=='O':\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'IsCriticalBlock')\n",
    "            elif winner==0 and sym=='X':\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'IsCriticalBlock')\n",
    "\n",
    "        for node_id in range(num_board_nodes):\n",
    "            for neighbor_id in edges[node_id]:\n",
    "                graphs.add_graph_node_edge(graph_id, node_id, neighbor_id, edge_type_name=0)\n",
    "\n",
    "    graphs.encode()\n",
    "    return graphs\n",
    "\n",
    "print(\"Initializing Tsetlin Machine...\")\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    args.number_of_clauses,\n",
    "    args.T,\n",
    "    args.s,\n",
    "    len(np.unique(y_train)),\n",
    "    depth=args.depth,\n",
    "    max_included_literals=args.max_included_literals,\n",
    "    message_size=args.message_size,\n",
    "    message_bits=args.message_bits\n",
    ")\n",
    "\n",
    "y_train = np.array(y_train,dtype=np.int32)\n",
    "y_test = np.array(y_test,dtype=np.int32)\n",
    "sp_train = np.array(sp_train)\n",
    "sp_test = np.array(sp_test)\n",
    "\n",
    "best_test_accuracy=0\n",
    "epochs_no_improve=0\n",
    "\n",
    "start_training = time()\n",
    "num_batches = int(np.ceil(len(X_train_df)/args.batch_size))\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
    "    indices = np.arange(len(X_train_df))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_df = X_train_df.iloc[indices].reset_index(drop=True)\n",
    "    y_train = y_train[indices]\n",
    "    sp_train = sp_train[indices]\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx*args.batch_size\n",
    "        end_idx = min((batch_idx+1)*args.batch_size,len(X_train_df))\n",
    "        X_batch_df = X_train_df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        y_batch = y_train[start_idx:end_idx]\n",
    "        sp_batch = sp_train[start_idx:end_idx]\n",
    "\n",
    "        graphs_batch = prepare_graph_data(X_batch_df, sp_batch, y_batch)\n",
    "        tm.fit(graphs_batch,y_batch,epochs=1,incremental=True)\n",
    "        del graphs_batch\n",
    "        gc.collect()\n",
    "\n",
    "    eval_indices = np.random.choice(len(X_train_df), size=2000, replace=False)\n",
    "    X_eval_df = X_train_df.iloc[eval_indices].reset_index(drop=True)\n",
    "    y_eval = y_train[eval_indices]\n",
    "    sp_eval = sp_train[eval_indices]\n",
    "\n",
    "    graphs_eval = prepare_graph_data(X_eval_df, sp_eval, y_eval)\n",
    "    train_predictions = tm.predict(graphs_eval)\n",
    "    train_accuracy = np.mean(y_eval==train_predictions)\n",
    "    del graphs_eval\n",
    "    gc.collect()\n",
    "\n",
    "    num_test_batches = int(np.ceil(len(X_test_df)/args.batch_size))\n",
    "    test_predictions=[]\n",
    "    for batch_idx in range(num_test_batches):\n",
    "        start_idx = batch_idx*args.batch_size\n",
    "        end_idx = min((batch_idx+1)*args.batch_size,len(X_test_df))\n",
    "        X_batch_df = X_test_df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        y_batch = y_test[start_idx:end_idx]\n",
    "        sp_batch = sp_test[start_idx:end_idx]\n",
    "\n",
    "        graphs_batch = prepare_graph_data(X_batch_df, sp_batch, y_batch)\n",
    "        preds = tm.predict(graphs_batch)\n",
    "        test_predictions.extend(preds)\n",
    "        del graphs_batch\n",
    "        gc.collect()\n",
    "\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_accuracy = np.mean(y_test==test_predictions)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    if test_accuracy>best_test_accuracy:\n",
    "        best_test_accuracy=test_accuracy\n",
    "        epochs_no_improve=0\n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "        if epochs_no_improve>=args.patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "stop_training = time()\n",
    "print(f\"\\nTraining Time: {stop_training - start_training:.2f} seconds\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test,test_predictions,digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test,test_predictions))\n",
    "\n",
    "unique_train_preds, counts_train_preds = np.unique(train_predictions, return_counts=True)\n",
    "print(\"\\nUnique predictions on training set:\", unique_train_preds)\n",
    "print(\"Training set predictions distribution:\", dict(zip(unique_train_preds, counts_train_preds)))\n",
    "\n",
    "unique_test_preds, counts_test_preds = np.unique(test_predictions, return_counts=True)\n",
    "print(\"Unique predictions on test set:\", unique_test_preds)\n",
    "print(\"Test set predictions distribution:\", dict(zip(unique_test_preds, counts_test_preds)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
