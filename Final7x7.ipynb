{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2587789-4723-4eab-ab7b-96299ad97998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "from collections import Counter\n",
    "import gc\n",
    "from GraphTsetlinMachine.graphs import Graphs\n",
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.epochs = 10\n",
    "        self.number_of_clauses = 30000\n",
    "        self.T = 20000\n",
    "        self.s = 5.0\n",
    "        self.depth = 15\n",
    "        self.hypervector_size = 8192\n",
    "        self.hypervector_bits = 8\n",
    "        self.message_size = 8192\n",
    "        self.message_bits = 8\n",
    "        self.double_hashing = True\n",
    "        self.max_included_literals = 256\n",
    "        self.batch_size = 500\n",
    "        self.patience = 10\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "args = Args()\n",
    "\n",
    "start_time = time()\n",
    "try:\n",
    "    data = pd.read_csv('datasett/hex_7x7_1M_movesBeforeEnd0.csv')\n",
    "    data = data.sample(10000, random_state=seed_value).reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset not found. Please ensure the dataset is in the correct directory.\")\n",
    "    exit(-1)\n",
    "end_time = time()\n",
    "print(f\"Loading data took {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Dataset size after sampling: {data.shape}\")\n",
    "\n",
    "board_size = 7\n",
    "cell_columns = [f'cell{row}_{col}' for row in range(board_size) for col in range(board_size)]\n",
    "\n",
    "required_columns = ['winner', 'starting_player'] + cell_columns\n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Error: Missing columns in the dataset: {missing_columns}\")\n",
    "    exit(-1)\n",
    "\n",
    "X_df = data[cell_columns]\n",
    "y = data['winner'].values.astype(int)\n",
    "starting_player = data['starting_player'].values.astype(int)\n",
    "\n",
    "if X_df.isnull().values.any():\n",
    "    print(\"Warning: Missing values detected in X_df. Filling missing values with 0.\")\n",
    "    X_df = X_df.fillna(0)\n",
    "\n",
    "unique_labels = np.unique(y)\n",
    "if not set(unique_labels).issubset({0, 1}):\n",
    "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y = np.array([label_mapping[label] for label in y])\n",
    "    print(\"Labels mapped to:\", label_mapping)\n",
    "\n",
    "def augment_data(X_df, y, sp):\n",
    "    X_aug = X_df.copy()\n",
    "    X_aug = X_aug.iloc[:, ::-1]\n",
    "    y_aug = y.copy()\n",
    "    sp_aug = sp.copy()\n",
    "    X_aug = X_aug.replace({1: -1, -1: 1})\n",
    "    return pd.concat([X_df, X_aug], ignore_index=True), np.concatenate([y, y_aug]), np.concatenate([sp, sp_aug])\n",
    "\n",
    "X_df, y, starting_player = augment_data(X_df, y, starting_player)\n",
    "print(f\"Dataset size after augmentation: {X_df.shape}\")\n",
    "label_counts = Counter(y)\n",
    "print(f\"Class distribution after augmentation: {label_counts}\")\n",
    "\n",
    "train_size = int(len(X_df) * 0.8)\n",
    "X_train_df = X_df.iloc[:train_size].reset_index(drop=True)\n",
    "y_train = y[:train_size]\n",
    "sp_train = starting_player[:train_size]\n",
    "X_test_df = X_df.iloc[train_size:].reset_index(drop=True)\n",
    "y_test = y[train_size:]\n",
    "sp_test = starting_player[train_size:]\n",
    "\n",
    "print(f\"X_train shape: {X_train_df.shape}\")\n",
    "print(f\"X_test shape: {X_test_df.shape}\")\n",
    "\n",
    "train_label_counts = Counter(y_train)\n",
    "min_class_size = min(train_label_counts.values())\n",
    "print(\"Training set class distribution before balancing:\", train_label_counts)\n",
    "class_indices = {cls: np.where(y_train == cls)[0] for cls in np.unique(y_train)}\n",
    "selected_indices = np.concatenate([\n",
    "    np.random.choice(indices, min_class_size, replace=False)\n",
    "    for indices in class_indices.values()\n",
    "])\n",
    "np.random.shuffle(selected_indices)\n",
    "\n",
    "X_train_df = X_train_df.iloc[selected_indices].reset_index(drop=True)\n",
    "y_train = y_train[selected_indices]\n",
    "sp_train = sp_train[selected_indices]\n",
    "\n",
    "print(\"Balanced training set class distribution:\", Counter(y_train))\n",
    "\n",
    "value_to_symbol = {1: 'X', -1: 'O', 0: 'Empty'}\n",
    "\n",
    "symbol_names = [\n",
    "    'X', 'O', 'Empty',\n",
    "    'StartingPlayer0', 'StartingPlayer1',\n",
    "    'Center', 'Edge', 'Corner',\n",
    "    'Bridge',\n",
    "    'IsCriticalBlock',\n",
    "    'Row_r0', 'Row_r1', 'Row_r2', 'Row_r3', 'Row_r4', 'Row_r5', 'Row_r6',\n",
    "    'Col_c0', 'Col_c1', 'Col_c2', 'Col_c3', 'Col_c4', 'Col_c5', 'Col_c6',\n",
    "    'DistFromCenter_Near', 'DistFromCenter_Mid', 'DistFromCenter_Far',\n",
    "    'NeighborX_Low', 'NeighborX_Medium', 'NeighborX_High',\n",
    "    'NeighborO_Low', 'NeighborO_Medium', 'NeighborO_High',\n",
    "    'TotalX_Low', 'TotalX_Medium', 'TotalX_High',\n",
    "    'TotalO_Low', 'TotalO_Medium', 'TotalO_High'\n",
    "]\n",
    "\n",
    "def prepare_graph_data(X_df_batch, sp_series_batch, y_batch):\n",
    "    num_graphs = X_df_batch.shape[0]\n",
    "    num_board_nodes = board_size ** 2\n",
    "    total_nodes_per_graph = num_board_nodes\n",
    "\n",
    "    graphs = Graphs(\n",
    "        number_of_graphs=num_graphs,\n",
    "        symbols=symbol_names,\n",
    "        hypervector_size=args.hypervector_size,\n",
    "        hypervector_bits=args.hypervector_bits,\n",
    "        double_hashing=args.double_hashing,\n",
    "    )\n",
    "\n",
    "    nodes = [(row, col) for row in range(board_size) for col in range(board_size)]\n",
    "    node_id_map = {(row, col): idx for idx, (row, col) in enumerate(nodes)}\n",
    "\n",
    "    directions = [\n",
    "        (-1, 0),   # North\n",
    "        (-1, 1),   # Northeast\n",
    "        (0, 1),    # East\n",
    "        (1, 0),    # South\n",
    "        (1, -1),   # Southwest\n",
    "        (0, -1),   # West\n",
    "    ]\n",
    "\n",
    "    edges = [[] for _ in range(total_nodes_per_graph)]\n",
    "    n_edges_list = [0 for _ in range(total_nodes_per_graph)]\n",
    "\n",
    "    for row, col in nodes:\n",
    "        node_id = node_id_map[(row, col)]\n",
    "        for dr, dc in directions:\n",
    "            neighbor_row = row + dr\n",
    "            neighbor_col = col + dc\n",
    "            if 0 <= neighbor_row < board_size and 0 <= neighbor_col < board_size:\n",
    "                neighbor_id = node_id_map[(neighbor_row, neighbor_col)]\n",
    "                edges[node_id].append(neighbor_id)\n",
    "                n_edges_list[node_id] += 1\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        graphs.set_number_of_graph_nodes(graph_id=graph_id, number_of_graph_nodes=total_nodes_per_graph)\n",
    "    graphs.prepare_node_configuration()\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        for k in range(total_nodes_per_graph):\n",
    "            graphs.add_graph_node(graph_id, k, n_edges_list[k])\n",
    "    graphs.prepare_edge_configuration()\n",
    "\n",
    "    for graph_id in range(num_graphs):\n",
    "        row_data = X_df_batch.iloc[graph_id]\n",
    "        board_state = row_data.values.astype(int)\n",
    "        board_state_symbols = [value_to_symbol.get(cell_value, 'Empty') for cell_value in board_state]\n",
    "        sp = sp_series_batch[graph_id]\n",
    "        winner = y_batch[graph_id]\n",
    "\n",
    "        board_state_dict = {(row, col): board_state_symbols[idx] for idx, (row, col) in enumerate(nodes)}\n",
    "\n",
    "        total_X = board_state_symbols.count('X')\n",
    "        total_O = board_state_symbols.count('O')\n",
    "\n",
    "        if total_X <= 16:\n",
    "            total_X_property = 'TotalX_Low'\n",
    "        elif total_X <= 32:\n",
    "            total_X_property = 'TotalX_Medium'\n",
    "        else:\n",
    "            total_X_property = 'TotalX_High'\n",
    "\n",
    "        if total_O <= 16:\n",
    "            total_O_property = 'TotalO_Low'\n",
    "        elif total_O <= 32:\n",
    "            total_O_property = 'TotalO_Medium'\n",
    "        else:\n",
    "            total_O_property = 'TotalO_High'\n",
    "\n",
    "        for idx, (row_idx, col_idx) in enumerate(nodes):\n",
    "            sym = board_state_symbols[idx]\n",
    "            graphs.add_graph_node_property(graph_id, idx, sym)\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'StartingPlayer{sp}')\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, total_X_property)\n",
    "            graphs.add_graph_node_property(graph_id, idx, total_O_property)\n",
    "\n",
    "            if (row_idx == board_size // 2) and (col_idx == board_size // 2):\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Center')\n",
    "            elif (row_idx == 0 or row_idx == board_size - 1) and (col_idx == 0 or col_idx == board_size - 1):\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Corner')\n",
    "            else:\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'Edge')\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'Row_r{row_idx}')\n",
    "            graphs.add_graph_node_property(graph_id, idx, f'Col_c{col_idx}')\n",
    "\n",
    "            dist_from_center = abs(row_idx - board_size // 2) + abs(col_idx - board_size // 2)\n",
    "            if dist_from_center <= 2:\n",
    "                dist_property = 'DistFromCenter_Near'\n",
    "            elif dist_from_center <= 4:\n",
    "                dist_property = 'DistFromCenter_Mid'\n",
    "            else:\n",
    "                dist_property = 'DistFromCenter_Far'\n",
    "            graphs.add_graph_node_property(graph_id, idx, dist_property)\n",
    "\n",
    "            neighbor_symbols = []\n",
    "            for dr, dc in directions:\n",
    "                neighbor_row = row_idx + dr\n",
    "                neighbor_col = col_idx + dc\n",
    "                if 0 <= neighbor_row < board_size and 0 <= neighbor_col < board_size:\n",
    "                    neighbor_sym = board_state_dict.get((neighbor_row, neighbor_col), 'Empty')\n",
    "                    neighbor_symbols.append(neighbor_sym)\n",
    "            num_neighbor_X = neighbor_symbols.count('X')\n",
    "            num_neighbor_O = neighbor_symbols.count('O')\n",
    "\n",
    "            if num_neighbor_X <= 2:\n",
    "                neighborX_property = 'NeighborX_Low'\n",
    "            elif num_neighbor_X <= 4:\n",
    "                neighborX_property = 'NeighborX_Medium'\n",
    "            else:\n",
    "                neighborX_property = 'NeighborX_High'\n",
    "\n",
    "            if num_neighbor_O <= 2:\n",
    "                neighborO_property = 'NeighborO_Low'\n",
    "            elif num_neighbor_O <= 4:\n",
    "                neighborO_property = 'NeighborO_Medium'\n",
    "            else:\n",
    "                neighborO_property = 'NeighborO_High'\n",
    "\n",
    "            graphs.add_graph_node_property(graph_id, idx, neighborX_property)\n",
    "            graphs.add_graph_node_property(graph_id, idx, neighborO_property)\n",
    "\n",
    "            current_player = sym\n",
    "            if current_player in ['X', 'O']:\n",
    "                for dr1, dc1 in directions:\n",
    "                    neighbor_row1 = row_idx + dr1\n",
    "                    neighbor_col1 = col_idx + dc1\n",
    "                    neighbor_row2 = row_idx + 2 * dr1\n",
    "                    neighbor_col2 = col_idx + 2 * dc1\n",
    "                    if (0 <= neighbor_row2 < board_size and 0 <= neighbor_col2 < board_size):\n",
    "                        sym1 = board_state_dict.get((neighbor_row1, neighbor_col1), 'Empty')\n",
    "                        sym2 = board_state_dict.get((neighbor_row2, neighbor_col2), 'Empty')\n",
    "                        if sym1 == 'Empty' and sym2 == current_player:\n",
    "                            graphs.add_graph_node_property(graph_id, idx, 'Bridge')\n",
    "                            break\n",
    "\n",
    "            if winner == 1 and sym == 'O':\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'IsCriticalBlock')\n",
    "            elif winner == 0 and sym == 'X':\n",
    "                graphs.add_graph_node_property(graph_id, idx, 'IsCriticalBlock')\n",
    "\n",
    "        for node_id in range(num_board_nodes):\n",
    "            for neighbor_id in edges[node_id]:\n",
    "                graphs.add_graph_node_edge(graph_id, node_id, neighbor_id, edge_type_name=0)\n",
    "\n",
    "    graphs.encode()\n",
    "    return graphs\n",
    "\n",
    "print(\"Initializing Tsetlin Machine...\")\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    args.number_of_clauses,\n",
    "    args.T,\n",
    "    args.s,\n",
    "    len(np.unique(y_train)),\n",
    "    depth=args.depth,\n",
    "    max_included_literals=args.max_included_literals,\n",
    "    message_size=args.message_size,\n",
    "    message_bits=args.message_bits,\n",
    ")\n",
    "\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "y_test = np.array(y_test, dtype=np.int32)\n",
    "sp_train = np.array(sp_train)\n",
    "sp_test = np.array(sp_test)\n",
    "\n",
    "best_test_accuracy = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "start_training = time()\n",
    "num_batches = int(np.ceil(len(X_train_df) / args.batch_size))\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{args.epochs}\")\n",
    "    indices = np.arange(len(X_train_df))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_df = X_train_df.iloc[indices].reset_index(drop=True)\n",
    "    y_train = y_train[indices]\n",
    "    sp_train = sp_train[indices]\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * args.batch_size\n",
    "        end_idx = min((batch_idx + 1) * args.batch_size, len(X_train_df))\n",
    "        X_batch_df = X_train_df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        y_batch = y_train[start_idx:end_idx]\n",
    "        sp_batch = sp_train[start_idx:end_idx]\n",
    "\n",
    "        graphs_batch = prepare_graph_data(X_batch_df, sp_batch, y_batch)\n",
    "        tm.fit(graphs_batch, y_batch, epochs=1, incremental=True)\n",
    "        del graphs_batch\n",
    "        gc.collect()\n",
    "\n",
    "    eval_indices = np.random.choice(len(X_train_df), size=5000, replace=False)\n",
    "    X_eval_df = X_train_df.iloc[eval_indices].reset_index(drop=True)\n",
    "    y_eval = y_train[eval_indices]\n",
    "    sp_eval = sp_train[eval_indices]\n",
    "\n",
    "    graphs_eval = prepare_graph_data(X_eval_df, sp_eval, y_eval)\n",
    "    train_predictions = tm.predict(graphs_eval)\n",
    "    train_accuracy = np.mean(y_eval == train_predictions)\n",
    "    del graphs_eval\n",
    "    gc.collect()\n",
    "\n",
    "    num_test_batches = int(np.ceil(len(X_test_df) / args.batch_size))\n",
    "    test_predictions = []\n",
    "    for batch_idx in range(num_test_batches):\n",
    "        start_idx = batch_idx * args.batch_size\n",
    "        end_idx = min((batch_idx + 1) * args.batch_size, len(X_test_df))\n",
    "        X_batch_df = X_test_df.iloc[start_idx:end_idx].reset_index(drop=True)\n",
    "        y_batch = y_test[start_idx:end_idx]\n",
    "        sp_batch = sp_test[start_idx:end_idx]\n",
    "\n",
    "        graphs_batch = prepare_graph_data(X_batch_df, sp_batch, y_batch)\n",
    "        preds = tm.predict(graphs_batch)\n",
    "        test_predictions.extend(preds)\n",
    "        del graphs_batch\n",
    "        gc.collect()\n",
    "\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_accuracy = np.mean(y_test == test_predictions)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= args.patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "stop_training = time()\n",
    "print(f\"\\nTraining Time: {stop_training - start_training:.2f} seconds\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_predictions, digits=4))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "\n",
    "unique_train_preds, counts_train_preds = np.unique(train_predictions, return_counts=True)\n",
    "print(\"\\nUnique predictions on training set:\", unique_train_preds)\n",
    "print(\"Training set predictions distribution:\", dict(zip(unique_train_preds, counts_train_preds)))\n",
    "\n",
    "unique_test_preds, counts_test_preds = np.unique(test_predictions, return_counts=True)\n",
    "print(\"Unique predictions on test set:\", unique_test_preds)\n",
    "print(\"Test set predictions distribution:\", dict(zip(unique_test_preds, counts_test_preds)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
